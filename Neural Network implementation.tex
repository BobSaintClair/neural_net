\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, tabto}

\title{Neural Network implementation}
\author{Martin Lazo}
\date{April 2022}
\begin{document}

\NumTabs{7}
\maketitle
\section{Notation}
\noindent
$\odot$ \tab Hadamard (element-wise) product\\
$\textbf{v}$ \tab Vector, vectors are denoted in lower case and bold\\
$\textbf{M}$ \tab Matrix, matrices are denoted in upper case and bold\\
$\textbf{v}_i$ \tab $i$th element of a vector $\textbf{v}$\\
$\textbf{M}_{i,j}$ \tab Element in the $i$th row and $j$th column of a matrix $\textbf{M}$\\
$\textbf{M}_{i,}$ \tab $i$th row of a matrix $\textbf{M}$\\
$\textbf{M}_{,i}$ \tab $i$th column of a matrix $\textbf{M}$\\
$\textbf{o}_i$ \tab Vector which preserves only the $i$th element of a vector as a scalar\\
$$
\textbf{o}_3\textbf{v}=
\begin{pmatrix}
0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}
=
c
$$
$\textbf{O}_i$ \tab Square matrix which preserves only the $i$th element of a vector\\
$$
\textbf{O}_3\textbf{v}=
\begin{pmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
c
\end{pmatrix}
$$
\\
Hidden layers are indexed from $0$ to $n$ with $n$ being the output layer and $0$ being the input layer.\\
\\
$\textbf{X}$ \tab Features\\
$\textbf{Y}$ \tab Labels\\
$\textbf{x}$ \tab Features data point\\
$\textbf{y}$ \tab Labels data point\\
$\textbf{b}^m$ \tab Bias vector of the $m$th layer\\
$\textbf{W}^m$ \tab Weights matrix of the $m$th layer\\
$\phi^m$ \tab Activation function of the $m$th layer, applied element-wise\\
${\phi^m}'$ \tab Derivative of the activation function of the $m$th layer\\
\pagebreak
\section{Backpropagation calculus}

Neural network is a function $\hat{y}: \mathbb{R}^a \rightarrow \mathbb{R}^b$ where $a$ and $b$ are positive integers.

\noindent 
Loss function $\ell_2$ for a set $S = \{(\textbf{y}_n, \textbf{x}_n) : 1 \le n \le N\}$ :
$$\ell_2 = \frac{1}{N}\sum_{(\textbf{y},\textbf{x})\in S}(\hat{y}(\textbf{x})-\textbf{y})^T(\hat{y}(\textbf{x})-\textbf{y})$$

\noindent 
The aim is to minimize $\ell_2$ with respect to weights and biases. This is done by iteratively updating weights using gradient descent:
$$(\textbf{W}^m_{i,j})_{n+1} = (\textbf{W}^m_{i,j})_{n} - \alpha\frac{\partial \ell_2}{\partial(\textbf{W}^m_{i,j})_{n}}$$
$$(\textbf{b}^m_{i})_{n+1} = (\textbf{b}^m_{i})_{n} - \alpha\frac{\partial \ell_2}{\partial(\textbf{b}^m_{i})_{n}}$$

\noindent
where $\alpha$ is the learning rate.
$$\frac{\partial \ell_2}{\partial\textbf{W}^m_{i,j}} = \frac{2}{N}\sum_{(\textbf{y},\textbf{x})\in S}(\hat{y}(\textbf{x})-\textbf{y})^T\frac{\partial \hat{y}}{\partial \textbf{W}^m_{i,j}}(\textbf{x})$$
$$\frac{\partial \ell_2}{\partial\textbf{b}^m_{i}} = \frac{2}{N}\sum_{(\textbf{y},\textbf{x})\in S}(\hat{y}(\textbf{x})-\textbf{y})^T\frac{\partial \hat{y}}{\partial \textbf{b}^m_{i}}(\textbf{x})$$
\noindent 
Neural network output for an input $\textbf{x}$:
$$\hat{y}(\textbf{x})=\phi^n(\textbf{W}^n\phi^{n-1}(..\phi^{0}(\textbf{W}^0\textbf{x}+\textbf{b}^0)..)+\textbf{b}^n)$$

\noindent 
Partial derivatives w.r.t. weights:
$$\frac{\partial \hat{y}}{\partial \textbf{W}^m_{i,j}}(\textbf{x})={\phi^n}'(..)\odot..\odot(\textbf{W}^{m+2}({\phi^{m+1}}'(..)\odot(\textbf{W}^{m+1}(\textbf{O}_i{\phi^{m}}'(..)\textbf{o}_j\phi^{m-1}(..)))))$$

$$\frac{\partial \hat{y}}{\partial \textbf{W}^0_{i,j}}(\textbf{x})={\phi^n}'(..)\odot..\odot(\textbf{W}^{2}({\phi^{1}}'(..)\odot(\textbf{W}^{1}(\textbf{O}_i{\phi^{0}}'(..)\textbf{x}_j))))$$

$$\frac{\partial \hat{y}}{\partial \textbf{W}^n_{i,j}}(\textbf{x})=\textbf{O}_i{\phi^{n}}'(..)\textbf{o}_j\phi^{n-1}(..)$$

\noindent 
Partial derivatives w.r.t. biases:
$$\frac{\partial \hat{y}}{\partial \textbf{b}^m_{i}}(\textbf{x})={\phi^n}'(..)\odot..\odot(\textbf{W}^{m+2}({\phi^{m+1}}'(..)\odot(\textbf{W}^{m+1}(\textbf{O}_i{\phi^{m}}'(..)))))$$

$$\frac{\partial \hat{y}}{\partial \textbf{b}^0_{i}}(\textbf{x})={\phi^n}'(..)\odot..\odot(\textbf{W}^{2}({\phi^{1}}'(..)\odot(\textbf{W}^{1}(\textbf{O}_i{\phi^{0}}'(..)))))$$

$$\frac{\partial \hat{y}}{\partial \textbf{b}^n_{i}}(\textbf{x})=\textbf{O}_i{\phi^{n}}'(..)$$
\end{document}